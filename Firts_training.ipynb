{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Firts_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw2SmNljFVPM",
        "outputId": "6614160e-673f-4187-c307-ba841a78b8e7"
      },
      "source": [
        "pip install tf-agents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/f5/4b5ddf7138d2fdaad2f7d44437372525859183cdac4ffad3fd86a94f8f52/tf_agents-0.8.0-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.19.5)\n",
            "Collecting tensorflow-probability==0.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/c0/d6a9212d3e74748474b59e077e85ca577308c808eee93f9d2e11c3f1cc16/tensorflow_probability-0.12.2-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 24.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.3->tf-agents) (57.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (0.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (0.1.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tensorflow-probability, tf-agents\n",
            "  Found existing installation: tensorflow-probability 0.12.1\n",
            "    Uninstalling tensorflow-probability-0.12.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.12.1\n",
            "Successfully installed tensorflow-probability-0.12.2 tf-agents-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYrihmSHNiXM"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import deque\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from tf_agents.networks import q_rnn_network\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import tensorflow as tf\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "import os\n",
        "import tempfile\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.utils import common\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from tf_agents.drivers import dynamic_episode_driver"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAalzpX6OK93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03f2438-f617-4bdf-c4f0-ba31bf2eb44c"
      },
      "source": [
        "### reading data from csv and saving to dicts\n",
        "\n",
        "\n",
        "days_prices = defaultdict(dict)\n",
        "counter = 0\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/dane_bull\"\n",
        "for files in os.listdir(path):\n",
        "    counter+=1\n",
        "    np_dict = defaultdict(dict)\n",
        "    prices = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/dane_bull/{files}\", index_col = 0)\n",
        "    prices = prices.drop(\"time\", axis=1)\n",
        "    l = len(prices.columns)\n",
        "    for k in range(0,l):\n",
        "        prices = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/dane_bull/{files}\", index_col = 0)\n",
        "        prices = prices.drop(\"time\", axis=1)\n",
        "        prices = prices.iloc[:, k:k+1]\n",
        "        for i in prices.columns:\n",
        "            prices[f\"mov_5{i}\"] = prices[f\"{i}\"].ewm(span=10, adjust=False).mean()\n",
        "            prices[f\"mov_20{i}\"] = prices[f\"{i}\"].rolling(window=60).mean()\n",
        "            prices[f\"mov_60{i}\"] = prices[f\"{i}\"].rolling(window=100).mean()\n",
        "            prices = prices.dropna()\n",
        "            prices[\"5-20\"] = np.round((prices[f\"mov_5{i}\"] - prices[f\"mov_20{i}\"]), 4)\n",
        "            scaler_5 = preprocessing.StandardScaler().fit(prices[\"5-20\"].values.reshape(-1,1))\n",
        "            prices[\"5-20\"] = scaler_5.transform(prices[\"5-20\"].values.reshape(-1,1))\n",
        "            prices[\"5-60\"] = np.round((prices[f\"mov_5{i}\"] - prices[f\"mov_60{i}\"]), 4)\n",
        "            scaler_5 = preprocessing.StandardScaler().fit(prices[\"5-60\"].values.reshape(-1,1))\n",
        "            prices[\"5-60\"] = scaler_5.transform(prices[\"5-60\"].values.reshape(-1,1))\n",
        "            prices[\"20-60\"] = np.round((prices[f\"mov_20{i}\"] - prices[f\"mov_60{i}\"]), 4)\n",
        "            scaler_5 = preprocessing.StandardScaler().fit(prices[\"20-60\"].values.reshape(-1,1))\n",
        "            prices[\"20-60\"] = scaler_5.transform(prices[\"20-60\"].values.reshape(-1,1))\n",
        "            if round(prices.iloc[:,0].std(), 7) == 0:\n",
        "              continue\n",
        "            np_dict[f\"np_diff_{i}\"] = np.array([prices[\"5-20\"].values.reshape(-1,1), prices[\"5-60\"].values.reshape(-1,1), prices[\"20-60\"].values.reshape(-1,1), prices[f\"{i}\"].values.reshape(-1,1) ])\n",
        "    days_prices[f\"np_dict_{counter}\"] = np_dict"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4ED5_DbOrZ5",
        "outputId": "44646ce9-7310-4ad8-f611-37b4afa24d23"
      },
      "source": [
        "### defining max length of training steps\n",
        "\n",
        "\n",
        "def max_steps_lenght(days_prices):\n",
        "    max_steps = 0\n",
        "    for i in days_prices.keys():\n",
        "        crypto_dict = days_prices[i].keys()\n",
        "        number_of_crypto = len(crypto_dict)\n",
        "        first_dict = list(crypto_dict)[0]\n",
        "        lenght = len(days_prices[i][\"np_diff_XLMUSDT\"][0])-60\n",
        "        lenght = lenght * (number_of_crypto) # timestamp\n",
        "        max_steps += lenght\n",
        "    return max_steps\n",
        "max_steps_lenght(days_prices)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1810842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH6eCSqRiTjM"
      },
      "source": [
        "### creating environment\n",
        "\n",
        "\n",
        "class CustomEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self, days_price, initial_balance, lookback_window_size):\n",
        "\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(self.lookback_window_size * 3 + 1,), dtype=np.float32, name='observation')\n",
        "\n",
        "        self.days_price = days_price\n",
        "\n",
        "        self.days_count = 1 \n",
        "\n",
        "        self.price_dict = days_price[f\"np_dict_{self.days_count}\"]\n",
        "\n",
        "        self.dict_names = list(self.price_dict.keys())\n",
        "\n",
        "        self.dict_count = -1\n",
        "\n",
        "        self.initial_balance = initial_balance\n",
        "\n",
        "        self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "\n",
        "        self.episode_ended = True\n",
        "\n",
        "        self.active_position = 0\n",
        "\n",
        "        self.action = 1\n",
        "\n",
        "        self.fees = 0.001\n",
        "\n",
        "        self.spread = 0.001\n",
        "\n",
        "        self.money = initial_balance\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.cum_reward = 0\n",
        "\n",
        "        self.crypto_held = 0\n",
        "\n",
        "        self.previous_money = initial_balance\n",
        "\n",
        "        self._state = False\n",
        "        \n",
        "        self.price_bought = 0\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self.episode_ended = False\n",
        "        self.current_step = 0\n",
        "        self.price_bought = 0\n",
        "        self.money = self.initial_balance\n",
        "        self.active_position = 0\n",
        "        self.action = 0\n",
        "        self.crypto_held = 0\n",
        "        self.dict_count += 1\n",
        "        if self.dict_count == len(self.dict_names):\n",
        "            self.days_count += 1\n",
        "            self.dict_count = 0\n",
        "            self.price_dict = self.days_price[f\"np_dict_{self.days_count}\"]\n",
        "            self.dict_names = list(self.price_dict.keys())\n",
        "            self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "        self.mov_price = self.price_dict[f\"{self.dict_names[self.dict_count]}\"]\n",
        "        self.previous_money = self.initial_balance\n",
        "        self.cum_reward = 0  ###Only now\n",
        "        self.pnl = 0\n",
        "\n",
        "        self.last = self.current_step + self.lookback_window_size\n",
        "\n",
        "        self._state = np.array(\n",
        "            [self.mov_price[0, self.current_step:self.last], self.mov_price[1, self.current_step:self.last],\n",
        "             self.mov_price[2, self.current_step:self.last]])\n",
        "\n",
        "        self._state = np.append(self._state, self.active_position)\n",
        "\n",
        "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "    def open_position(self, action, current_price):\n",
        "        reward = 0\n",
        "        if (self.active_position == 0):\n",
        "            if action == 2:\n",
        "                self.active_position = 1\n",
        "                if self.money <= 0.8 * self.initial_balance:\n",
        "                    self.episode_ended = True\n",
        "                    reward = -200\n",
        "                    print('ERROR NO MONEY!')\n",
        "                else:\n",
        "                    self.money = self.money - (self.money * self.spread) - (self.money * self.fees)\n",
        "                    self.crypto_held = (self.money / current_price)\n",
        "                    reward = self.money - self.previous_money\n",
        "            \n",
        "        elif (self.active_position == 1):\n",
        "            if action == 0:\n",
        "                self.active_position = 0\n",
        "                self.money = self.crypto_held * current_price - (self.money * self.spread) - (self.money * self.fees)\n",
        "                reward = self.money - self.previous_money\n",
        "                self.crypto_held = 0\n",
        "            else:\n",
        "                self.money = self.crypto_held * current_price\n",
        "                reward = self.money - self.previous_money\n",
        "        else:\n",
        "            reward = self.money - self.previous_money\n",
        "\n",
        "        return reward  # think about money balance\n",
        "\n",
        "    def next_observation(self):\n",
        "\n",
        "        self.last = self.current_step + self.lookback_window_size\n",
        "        previous_price = self.mov_price[3, self.lookback_window_size + self.current_step - 1]\n",
        "        current_price = self.mov_price[3, self.lookback_window_size + self.current_step]\n",
        "        obs = np.array([self.mov_price[0, self.current_step:self.last], self.mov_price[1, self.current_step:self.last],\n",
        "                        self.mov_price[2, self.current_step:self.last]])\n",
        "        self.current_step += 1\n",
        "        return obs, current_price, previous_price \n",
        "\n",
        "    def _step(self, action):\n",
        "        reward = 0\n",
        "        if self.episode_ended == True:\n",
        "            return self._reset()\n",
        "        else :\n",
        "            obs, current_price, previous_price = self.next_observation()\n",
        "            self._state = obs\n",
        "\n",
        "        if (action == 0):\n",
        "          if self.active_position == 0:\n",
        "              reward = -3\n",
        "          elif self.active_position == 1:\n",
        "              reward = self.open_position(action, current_price)\n",
        "        elif (action == 2):\n",
        "          if self.active_position == 1:\n",
        "            reward = self.open_position(action, current_price) - 3\n",
        "          else: \n",
        "            reward = self.open_position(action, current_price)\n",
        "        elif (action == 1):\n",
        "            if self.active_position == 1:\n",
        "                self.money = self.crypto_held * current_price\n",
        "                reward = self.money - self.previous_money \n",
        "            elif self.active_position == 0:\n",
        "              reward = -1\n",
        "\n",
        "\n",
        "        if self.last == self.last_step: ##################################################################check should be if last = last.step -1\n",
        "            self.episode_ended = True\n",
        "\n",
        "        if self.episode_ended == True:\n",
        "            self.money = self.crypto_held * current_price - (self.money * self.spread) - (self.money * self.fees)\n",
        "            self.active_position = 0\n",
        "            reward = self.money - self.initial_balance\n",
        "\n",
        "        pnl = (self.money - self.previous_money)\n",
        "        self.previous_money = self.money\n",
        "        self.pnl += pnl\n",
        "        info = {}\n",
        "        self._state = np.append(self._state, self.active_position)\n",
        "        # return self._state, reward, done, info\n",
        "        if self.episode_ended == True:\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), float(reward))\n",
        "        else:\n",
        "            return ts.transition(\n",
        "                np.array(self._state, dtype=np.float32), reward=float(reward), discount=1.05)\n",
        "\n",
        "    def render(self):\n",
        "        print(f'Step: {self.current_step}, Money: {self.money}, PnL{self.pnl}, Cum reward{self.cum_reward}')\n",
        "    #####SKALIBROWAĆ REWARD\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TgV3mmmEqv7"
      },
      "source": [
        "#### creating model and agent\n",
        "\n",
        "\n",
        "class Agent_net():\n",
        "\n",
        "    def __init__(self, env):\n",
        "        # defining hyperparameters\n",
        "        self.env = env\n",
        "\n",
        "        self.train_env = tf_py_environment.TFPyEnvironment(self.env)\n",
        "        self.eval_env = tf_py_environment.TFPyEnvironment(self.env)\n",
        "        # network configuration\n",
        "        self.input_fc_layer_params = (181,)\n",
        "        self.lstm_size = (60,)\n",
        "        self.output_fc_layer_params = (60,)\n",
        "        self.learning_rate = 0.005\n",
        "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        self.td_errors_loss_fn = common.element_wise_squared_loss\n",
        "        self.train_step_counter = tf.compat.v1.train.get_or_create_global_step() #tf.Variable(0)  \n",
        "        self.replay_buffer_max_length = 1000000\n",
        "        self.batch_size = 64\n",
        "        self.epsilon = 0.2\n",
        "\n",
        "    def qrnn(self):\n",
        "        # create a q_RNNnet\n",
        "        self.q_net = q_rnn_network.QRnnNetwork(\n",
        "            self.train_env.observation_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            input_fc_layer_params=self.input_fc_layer_params,\n",
        "            lstm_size=self.lstm_size,\n",
        "            output_fc_layer_params=self.output_fc_layer_params)\n",
        "\n",
        "    def agent_create(self):\n",
        "        self.qrnn()\n",
        "        self.target = self.qrnn()\n",
        "        self.agent = dqn_agent.DqnAgent(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            q_network=self.q_net,\n",
        "            target_q_network = self.target,\n",
        "            target_update_tau = 0.1,\n",
        "            target_update_period = 10,\n",
        "            epsilon_greedy = self.epsilon,\n",
        "            optimizer=self.optimizer,\n",
        "            td_errors_loss_fn=self.td_errors_loss_fn,\n",
        "            train_step_counter=self.train_step_counter)\n",
        "\n",
        "        self.agent.initialize() # just to start agent\n",
        "        self.eval_policy = self.agent.policy # return the policy\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(self.train_env.time_step_spec(),\n",
        "                                                             self.train_env.action_spec())\n",
        "        return (self.agent, self.train_env, self.eval_env)\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEbTvZ-Md7K3"
      },
      "source": [
        "def r_buffer(agent, train_env, replay_buffer_max_length):\n",
        "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "        data_spec=agent.collect_data_spec, # getting data scpecification \n",
        "        batch_size=train_env.batch_size, # how many experience there are in a batch\n",
        "        max_length=replay_buffer_max_length) # maximum capacity of buffer is max_lenght * batch size (stored in a tuple)\n",
        "    return replay_buffer"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UpNQPW4ST6T"
      },
      "source": [
        "def driver_collect(replay_buffer, train_env, agent):\n",
        "  replay_buffer.clear()\n",
        "  observers = [replay_buffer.add_batch]       \n",
        "  driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "            train_env, agent.collect_policy, observers, num_episodes=1)\n",
        "  driver.run()\n",
        "  return replay_buffer\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGBKP_Eld9xk"
      },
      "source": [
        "def iter_data(replay_buffer, train_env):\n",
        "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "  ns = replay_buffer.num_frames()\n",
        "  ns = ns.numpy()\n",
        "  dataset = replay_buffer.as_dataset(\n",
        "      num_parallel_calls=AUTOTUNE, #how many process at time, if none sequential\n",
        "      sample_batch_size=train_env.batch_size, #how many batches to get\n",
        "      num_steps=ns).prefetch(AUTOTUNE) #how many steps in a batch  \n",
        "  iterator = iter(dataset)\n",
        "  return (iterator)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJjWDh30O58x"
      },
      "source": [
        "### computing reward returned in order to work collect_step() must be first\n",
        "\n",
        "\n",
        "\n",
        "def compute_avg_return(environment, policy, num_eval_episodes):\n",
        "\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_eval_episodes): # number o evaluation episodes\n",
        "\n",
        "        time_step = environment.reset() # getting first time_step\n",
        "        episode_return = 0\n",
        "        policy_state = policy.get_initial_state(batch_size=env.batch_size)\n",
        "        avg_return = 0\n",
        "        \n",
        "\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step, policy_state) # action taken for given timestep\n",
        "            time_step = environment.step(action_step.action) # assign next time_step \n",
        "            episode_return += time_step.reward # reward taken from next time_step ?\n",
        "           \n",
        "        total_return+= episode_return\n",
        "        print(episode_return)\n",
        "    avg_return = total_return  / num_eval_episodes\n",
        "    return avg_return"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmjp89p2R8N-"
      },
      "source": [
        "# agent=None\n",
        "# env = None\n",
        "# tf.compat.v1.reset_default_graph()\n",
        "# env = CustomEnv(days_prices, 1000, 60)\n",
        "# na = Agent_net(env)\n",
        "# agent, train_env, eval_env = na.agent_create()\n",
        "# replay_buffer = r_buffer(agent, train_env, 500000)\n",
        "# count = 0\n",
        "# for _ in range(119):  #######docelowo 120*len(days_prices.keys())\n",
        "#       count+=1\n",
        "#       replay_buffer = driver_collect(replay_buffer, train_env, agent)\n",
        "#       iterator = iter_data(replay_buffer, train_env)\n",
        "#       print(iterator.get_next())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAcomA01E4xc"
      },
      "source": [
        "### training\n",
        "\n",
        "\n",
        "def training(agent, train_env, replay_buffer, eval_env, iterator):\n",
        "    num_iterations = 1000000  \n",
        "    collect_steps_per_iteration = 10 \n",
        "    log_interval = 30\n",
        "    eval_interval = 100\n",
        "    num_eval_episodes = 3\n",
        "    index_out = False\n",
        "    n_episodes = 120 * len(days_prices.keys())\n",
        "    try:\n",
        "        % % time # used for prolonged training\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "    agent.train = common.function(agent.train)\n",
        "\n",
        "    # Evaluate the agent's policy once before training.\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes) #use created function\n",
        "    returns = [avg_return]\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "          try:\n",
        "            replay_buffer = driver_collect(replay_buffer, train_env, agent)\n",
        "            iterator = iter_data(replay_buffer, train_env)\n",
        "            \n",
        "\n",
        "            # Sample a batch of data from the buffer and update the agent's network.\n",
        "            experience, unused_info = iterator.get_next() \n",
        "            train_loss = agent.train(experience).loss\n",
        "          except IndexError:\n",
        "            index_out = True\n",
        "\n",
        "          step = agent.train_step_counter.numpy()\n",
        "\n",
        "          if step % log_interval == 0:\n",
        "              print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "              \n",
        "          if step % eval_interval == 0:\n",
        "              try:\n",
        "                  avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "                  print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "                  returns.append(avg_return)\n",
        "              except Exception:\n",
        "                  print(\"index error\")\n",
        "                  index_out = True\n",
        "          if index_out == True:\n",
        "            break\n",
        "\n",
        "    return agent.train_step_counter, agent, replay_buffer"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OJx_i18PB2W",
        "outputId": "6618b66d-1dfe-44ff-acdc-effdee31cd97"
      },
      "source": [
        "### calling env, agent creation, training etc\n",
        "agent=None\n",
        "env = None\n",
        "tf.compat.v1.reset_default_graph()\n",
        "env = CustomEnv(days_prices, 1000, 60)\n",
        "#utils.validate_py_environment(env, episodes=1)\n",
        "na = Agent_net(env)\n",
        "agent, train_env, eval_env = na.agent_create()\n",
        "replay_buffer = r_buffer(agent, train_env, 50000)\n",
        "iterator = iter_data(replay_buffer, train_env)\n",
        "save_train_step_counter, save_agent, save_replay_buffer = training(agent, train_env, replay_buffer, eval_env, iterator)\n",
        "\n",
        "chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies\"\n",
        "policy_dir = os.path.join(chkpdir, 'policy')\n",
        "checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    model = na.q_net,\n",
        "    agent=save_agent,\n",
        "    policy=save_agent.policy,\n",
        "    replay_buffer=save_replay_buffer,\n",
        "    global_step=save_agent.train_step_counter\n",
        ")\n",
        "\n",
        "tf_policy_saver = policy_saver.PolicySaver(save_agent.policy)\n",
        "checkpoint = train_checkpointer.save(save_agent.train_step_counter)\n",
        "#train_checkpointer.initialize_or_restore()\n",
        "#global_step = tf.compat.v1.train.get_global_step()\n",
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([-2258.629], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-2240.5103], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-2196.9285], shape=(1,), dtype=float32)\n",
            "step = 30: loss = 1002062.3125\n",
            "step = 60: loss = 998974.25\n",
            "step = 90: loss = 2814.17236328125\n",
            "tf.Tensor([-23.262459], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-181.70383], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-140.60687], shape=(1,), dtype=float32)\n",
            "step = 100: Average Return = [-115.19105]\n",
            "step = 120: loss = 8380.2138671875\n",
            "step = 150: loss = 20773.23828125\n",
            "step = 180: loss = 20365.13671875\n",
            "ERROR NO MONEY!\n",
            "tf.Tensor([-1009.04846], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-1009.7992], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-388.26544], shape=(1,), dtype=float32)\n",
            "step = 200: Average Return = [-802.37103]\n",
            "ERROR NO MONEY!\n",
            "ERROR NO MONEY!\n",
            "step = 210: loss = 999594.75\n",
            "ERROR NO MONEY!\n",
            "ERROR NO MONEY!\n",
            "ERROR NO MONEY!\n",
            "ERROR NO MONEY!\n",
            "step = 240: loss = 905.3056640625\n",
            "step = 270: loss = 4956.59326171875\n",
            "step = 300: loss = 998563.375\n",
            "tf.Tensor([-1470.], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-1470.], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-1470.], shape=(1,), dtype=float32)\n",
            "step = 300: Average Return = [-1470.]\n",
            "step = 330: loss = 11612.201171875\n",
            "step = 360: loss = 542.0897216796875\n",
            "step = 390: loss = 2348.954345703125\n",
            "tf.Tensor([-1232.], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-1232.], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-1232.], shape=(1,), dtype=float32)\n",
            "step = 400: Average Return = [-1232.]\n",
            "ERROR NO MONEY!\n",
            "ERROR NO MONEY!\n",
            "step = 420: loss = 27238.673828125\n",
            "ERROR NO MONEY!\n",
            "ERROR NO MONEY!\n",
            "step = 450: loss = 1015810.9375\n",
            "ERROR NO MONEY!\n",
            "step = 480: loss = 990391.5625\n",
            "tf.Tensor([-453.6848], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-493.04794], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-515.60236], shape=(1,), dtype=float32)\n",
            "step = 500: Average Return = [-487.44507]\n",
            "step = 510: loss = 2218.09423828125\n",
            "step = 540: loss = 9732.1611328125\n",
            "step = 570: loss = 6920.7158203125\n",
            "ERROR NO MONEY!\n",
            "step = 600: loss = 985641.5625\n",
            "tf.Tensor([-1421.], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-1421.], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-1421.], shape=(1,), dtype=float32)\n",
            "step = 600: Average Return = [-1421.]\n",
            "step = 630: loss = 991889.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkVPodEJWDyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e07b8a-0f42-4a11-be44-a978d9cbcb41"
      },
      "source": [
        "chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies\"\n",
        "policy_dir = os.path.join(chkpdir, 'policy')\n",
        "checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    model = na.q_net,\n",
        "    agent=save_agent,\n",
        "    policy=save_agent.policy,\n",
        "    global_step=save_agent.train_step_counter\n",
        ")\n",
        "\n",
        "tf_policy_saver = policy_saver.PolicySaver(save_agent.policy)\n",
        "checkpoint = train_checkpointer.save(save_agent.train_step_counter)\n",
        "#train_checkpointer.initialize_or_restore()\n",
        "#global_step = tf.compat.v1.train.get_global_step()\n",
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation, 1/0, 1/1 with unsupported characters which will be renamed to step_type, reward, discount, observation, unknown, unknown_0 in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as QRnnNetwork_layer_call_and_return_conditional_losses, QRnnNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dynamic_unroll_1_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/policies/policy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/policies/policy/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EB8neS-4skg"
      },
      "source": [
        "policy = save_agent.policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_dL1rY2XoRcF",
        "outputId": "f3534abe-362c-41fb-a0f8-39d337c4891f"
      },
      "source": [
        "for d in days_prices.keys():\n",
        "  PnL = 0\n",
        "  for k in np_dict.keys():\n",
        "    balance = 1000\n",
        "    spread = 0.001\n",
        "    fees = 0.001\n",
        "    crypto_held = 0\n",
        "    active_position = 0\n",
        "    policy_state = policy.get_initial_state(1)\n",
        "    a_count = 0\n",
        "    pos = 0\n",
        "    for i in range(0, 100):\n",
        "        price_mov_data = np_dict[f\"{k}\"][0:3, i:i + 60].reshape(180, )\n",
        "        price = np_dict[f\"{k}\"][3, i]\n",
        "        \n",
        "        \n",
        "\n",
        "        price_mov_data = np.append([price_mov_data], [active_position])\n",
        "\n",
        "        ts_obs = tf.convert_to_tensor(price_mov_data)\n",
        "\n",
        "        step_type = tf.convert_to_tensor(\n",
        "            [0], dtype=tf.int32, name='time_step')\n",
        "        reward = tf.convert_to_tensor(\n",
        "            [0], dtype=tf.float32, name='reward')\n",
        "        discount = tf.convert_to_tensor(\n",
        "            [0], dtype=tf.float32, name='discount')\n",
        "        observations = tf.convert_to_tensor(\n",
        "            [ts_obs], dtype=tf.float32, name='observations') \n",
        "        timestep = ts.TimeStep(step_type, reward, discount, observations)\n",
        "\n",
        "        # def runPolicy(timestep, _policy, _policy_state, balance, fees, crypto_held, active_position, price):\n",
        "\n",
        "        a = policy.action(timestep, policy_state)\n",
        "        policy_state = a.state\n",
        "        a = a.action.numpy()\n",
        "        #a = random.randint(0,2)\n",
        "        #a=1\n",
        "        #print(a)\n",
        "      \n",
        "        if active_position == 0:\n",
        "            if int(a) == 1:\n",
        "                active_position = 1\n",
        "                if balance <= 1:\n",
        "                    print('ERROR NO MONEY!')\n",
        "                else:\n",
        "                    balance = balance - (balance * spread) - (balance * fees)\n",
        "                    crypto_held = (balance / price)\n",
        "        elif active_position == 1:\n",
        "            if int(a) == 0:\n",
        "                active_position = 0\n",
        "                balance = crypto_held * price - (balance * spread) - (balance * fees)\n",
        "                crypto_held = 0\n",
        "            else:\n",
        "                balance = crypto_held * price\n",
        "        else:\n",
        "            balance = crypto_held * price\n",
        "        \n",
        "        if int(a) == 1:\n",
        "          a_count+= 1\n",
        "\n",
        "        pnl = 1000 - balance\n",
        "      \n",
        "        \n",
        "        \n",
        "    \n",
        "    PnL += -pnl\n",
        "    print(-pnl, a_count)\n",
        "\n",
        "  print(PnL)\n",
        "# runPolicy(timestep, policy, policy_state, balance, fees, crypto_held, active_position, price)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0\n",
            "[1.13295922] 25\n",
            "[9.67759023] 30\n",
            "[12.85416667] 41\n",
            "0 0\n",
            "[22.36008345] 25\n",
            "[13.48324459] 26\n",
            "0 0\n",
            "[69.3095996] 53\n",
            "0 0\n",
            "[-3.24103627] 17\n",
            "[-1.09895269] 43\n",
            "[6.99639423] 20\n",
            "[11.94083813] 22\n",
            "[-3.7204693] 24\n",
            "[53.2515708] 51\n",
            "[40.2331012] 39\n",
            "[-5.12363067] 18\n",
            "[3.6572367] 21\n",
            "[14.61262507] 22\n",
            "0 0\n",
            "[-3.61265005] 21\n",
            "0 0\n",
            "[0.37069989] 21\n",
            "[2.87870455] 19\n",
            "[-2.24818462] 24\n",
            "[-0.96201689] 20\n",
            "[-3.81950775] 18\n",
            "[2.24476662] 24\n",
            "[-6.06194812] 21\n",
            "[5.51113138] 24\n",
            "[-5.57108779] 25\n",
            "[12.1502937] 25\n",
            "[-12.53115613] 19\n",
            "0 0\n",
            "[-9.33978836] 100\n",
            "[2.93652102] 22\n",
            "[-4.47816464] 26\n",
            "0 0\n",
            "[-13.50497288] 24\n",
            "[5.83498053] 21\n",
            "[4.95999161] 24\n",
            "[14.77532425] 22\n",
            "[-5.23594945] 18\n",
            "[6.42215894] 22\n",
            "[7.72849626] 22\n",
            "[6.87111111] 23\n",
            "[-2.4932536] 24\n",
            "[0.19555121] 18\n",
            "[9.05930638] 24\n",
            "[-3.23082833] 19\n",
            "[38.86058429] 36\n",
            "[-5.34395644] 26\n",
            "0 0\n",
            "[10.59867342] 25\n",
            "[14.89550496] 80\n",
            "[2.81078504] 21\n",
            "[0.37392959] 21\n",
            "[-3.34798977] 25\n",
            "[-3.89434254] 18\n",
            "[-2.9266481] 18\n",
            "[1.43537982] 20\n",
            "0 0\n",
            "[-4.73866021] 40\n",
            "[-10.95347382] 20\n",
            "[-3.31801104] 23\n",
            "[0.63884578] 100\n",
            "[0.12896512] 70\n",
            "[-1.17053268] 18\n",
            "[-0.978157] 4\n",
            "[1.44280882] 23\n",
            "[-4.87023211] 26\n",
            "[-5.67346429] 100\n",
            "[6.41233433] 100\n",
            "[-0.97191741] 21\n",
            "[4.87646341] 27\n",
            "[17.82142543] 29\n",
            "[2.11899168] 15\n",
            "[3.87610909] 100\n",
            "[-3.92003346] 21\n",
            "[-0.37617963] 20\n",
            "[46.37799564] 29\n",
            "0 0\n",
            "0 0\n",
            "[7.15886076] 20\n",
            "[-4.10260479] 17\n",
            "[2.26756261] 23\n",
            "[-4.40452215] 15\n",
            "[-3.80694806] 23\n",
            "[-4.17844475] 18\n",
            "[-5.29786006] 22\n",
            "[10.28231602] 16\n",
            "[1.44360044] 22\n",
            "[1.68796192] 18\n",
            "[7.82969707] 23\n",
            "0 0\n",
            "[-3.71671236] 15\n",
            "0 0\n",
            "0 0\n",
            "[-0.03080457] 100\n",
            "[31.32842732] 21\n",
            "[-0.67419462] 23\n",
            "0 0\n",
            "[-30.04777677] 100\n",
            "[9.14247625] 21\n",
            "[0.6181441] 21\n",
            "[-21.70085429] 100\n",
            "[0.52196022] 100\n",
            "[8.74679119] 22\n",
            "[9.06775128] 19\n",
            "[7.11903766] 21\n",
            "[10.0527202] 20\n",
            "[-13.1295867] 100\n",
            "[5.37514509] 26\n",
            "[2.53934678] 21\n",
            "[33.84746597] 28\n",
            "[-11.62922123] 17\n",
            "[-8.50015508] 18\n",
            "[9.46113074] 25\n",
            "0 0\n",
            "[402.63075797]\n",
            "0 0\n",
            "[1.13295922] 25\n",
            "[9.67759023] 30\n",
            "[12.85416667] 41\n",
            "0 0\n",
            "[22.36008345] 25\n",
            "[13.48324459] 26\n",
            "0 0\n",
            "[69.3095996] 53\n",
            "0 0\n",
            "[-3.24103627] 17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-4e54ebb2e111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# def runPolicy(timestep, _policy, _policy_state, balance, fees, crypto_held, active_position, price):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         distribution_step.action)\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         distribution_step.action)\n\u001b[1;32m    564\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/distributions/reparameterized_sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(distribution, reparam, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m       \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mprepended\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \"\"\"\n\u001b[0;32m-> 1002\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_sample_n\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must provide JAX PRNGKey as `dist.sample(seed=.)`'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m       sample_shape = ps.convert_to_shape_tensor(\n\u001b[0;32m--> 976\u001b[0;31m           ps.cast(sample_shape, tf.int32), name='sample_shape')\n\u001b[0m\u001b[1;32m    977\u001b[0m       sample_shape, n = self._expand_sample_shape_to_vector(\n\u001b[1;32m    978\u001b[0m           sample_shape, 'sample_shape')\n",
            "\u001b[0;32m<decorator-gen-140>\u001b[0m in \u001b[0;36mcast\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/prefer_static.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     73\u001b[0m                      for arg, arg_ in zip(flat_args, flat_args_))\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_static\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstatic_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \"\"\"\n\u001b[0;32m--> 755\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m     raise TypeError(\n\u001b[1;32m    615\u001b[0m         \u001b[0;34m\"Attempted to pack value:\\n  {}\\ninto a sequence, but found \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9SnO2eeoTl5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}